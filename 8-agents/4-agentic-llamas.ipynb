{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Building Agents with LangGraph & Llama Stack\n",
    "\n",
    "In the last notebook, you built a ReAct agent **from scratch**. You wrote:\n",
    "- Manual response parsing with regex\n",
    "- Custom iteration loops\n",
    "- State management\n",
    "- Tool execution logic\n",
    "- Error handling\n",
    "\n",
    "That was ~200+ lines of code. Now let's see how **LangGraph** makes this dramatically simpler.\n",
    "\n",
    "## What We'll Build\n",
    "\n",
    "A knowledge-based assistant that:\n",
    "1. Searches documents for answers (RAG)\n",
    "2. Schedules meetings with professors if it can't answer\n",
    "\n",
    "Same capabilities. Way less code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q langgraph==0.6.7 langchain-openai==0.3.32 langchain-core==0.3.75 llama-stack-client==0.3.0 langchain==0.3.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d99ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disable some logs\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connect",
   "metadata": {},
   "source": [
    "## Connect to Llama Stack\n",
    "\n",
    "LangGraph connects to Llama Stack using the **Responses API**, which provides enhanced functionality including direct MCP tool binding.\n",
    "\n",
    "**Important**: When using `use_responses_api=True`, the base URL should be:\n",
    "- ‚úÖ `http://llama-stack-service:8321/v1` (Responses API endpoint)\n",
    "- ‚ùå NOT `http://llama-stack-service:8321/v1/openai/v1` (that's for standard Chat Completions)\n",
    "\n",
    "The Responses API is what enables the seamless MCP integration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "client",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph connects via Llama Stack Responses API\n",
    "# When using use_responses_api=True, the base should be /v1 (not /v1/openai/v1)\n",
    "# The Responses API supports tool binding for MCP integration\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_base=\"http://llama-stack-service:8321/v1\",\n",
    "    model=\"llama32\",\n",
    "    openai_api_key=\"not-needed\",\n",
    "    use_responses_api=True,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "# Also connect standard client for RAG\n",
    "llama_client = LlamaStackClient(base_url=\"http://llama-stack-service:8321\")\n",
    "\n",
    "print(f\"‚úÖ Connected to Llama Stack with Responses API enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-section",
   "metadata": {},
   "source": [
    "## Define Tools\n",
    "\n",
    "We'll give the agent access to:\n",
    "1. **RAG Search** - Search the knowledge base for information\n",
    "2. **Professor Directory** - Look up available professors by expertise  \n",
    "3. **MCP Calendar Tools** - Direct MCP server integration!\n",
    "\n",
    "For the MCP calendar integration, we use Llama Stack's Responses API which supports **direct tool binding**. We simply include the MCP binding in the tools list, and LangGraph handles everything automatically:\n",
    "\n",
    "- The MCP binding connects to the real calendar server you configured\n",
    "- All 9 calendar tools become available (create_event, get_upcoming_events, search_events, etc.)\n",
    "- Tool calls are handled asynchronously through Llama Stack\n",
    "- No manual wrapper functions needed!\n",
    "\n",
    "This is the same MCP calendar server you used in previous notebooks (`2-mcp-servers.ipynb` and `3-agentic-workflows.ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's get our vector store for RAG\n",
    "try:\n",
    "    vector_stores = llama_client.vector_stores.list()\n",
    "    \n",
    "    if vector_stores.data and len(vector_stores.data) > 0:\n",
    "        vector_store = vector_stores.data[0]\n",
    "        print(f\"‚úÖ Using vector store: {vector_store.id}\")\n",
    "    else:\n",
    "        raise Exception(\"Can't find an existing vector store\")\n",
    "\n",
    "    # Delete and recreate the vector store so that it properly connects to the underlying vector db after restarting LLS pod\n",
    "    llama_client.vector_stores.delete(vector_store_id=vector_store.id)\n",
    "    vector_store = llama_client.vector_stores.create(\n",
    "        name=\"my_citations_db\",\n",
    "        extra_body={\n",
    "            \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "            \"embedding_dimension\": 384,\n",
    "            \"provider_id\": \"milvus\",\n",
    "            \"vector_db_id\": \"test\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Vector store setup failed: {e}\")\n",
    "    vector_store = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tools",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Professor directory - in a real system this would be a database\n",
    "PROFESSORS = {\n",
    "    \"Dr. Sarah Chen\": {\n",
    "        \"department\": \"Computer Science\",\n",
    "        \"expertise\": [\"Machine Learning\", \"Neural Networks\", \"AI Ethics\", \"Agentic Workflows\"],\n",
    "        \"email\": \"s.chen@university.edu\"\n",
    "    },\n",
    "    \"Prof. Michael Rodriguez\": {\n",
    "        \"department\": \"Physics\",\n",
    "        \"expertise\": [\"Quantum Mechanics\", \"Particle Physics\", \"Quantum Chromodynamics\"],\n",
    "        \"email\": \"m.rodriguez@university.edu\"\n",
    "    },\n",
    "    \"Dr. Emily Thompson\": {\n",
    "        \"department\": \"Biology\",\n",
    "        \"expertise\": [\"Botany\", \"Ecology\", \"Forest Canopy Structure\", \"Plant Biology\"],\n",
    "        \"email\": \"e.thompson@university.edu\"\n",
    "    },\n",
    "    \"Prof. James Wilson\": {\n",
    "        \"department\": \"Computer Science\",\n",
    "        \"expertise\": [\"Distributed Systems\", \"Cloud Computing\", \"Software Architecture\"],\n",
    "        \"email\": \"j.wilson@university.edu\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_knowledge_base(query: str) -> str:\n",
    "    \"\"\"Search through documents to find information. Use this when the user asks about concepts, definitions, or topics.\"\"\"\n",
    "    if not vector_store:\n",
    "        return \"Error: Knowledge base not available. Please set up the vector store first.\"\n",
    "    \n",
    "    try:\n",
    "        results = llama_client.vector_stores.search(\n",
    "            vector_store_id=vector_store.id,\n",
    "            query=query,\n",
    "            max_num_results=3,\n",
    "            search_mode=\"vector\"\n",
    "        )\n",
    "        \n",
    "        if not results.data:\n",
    "            return \"No relevant information found in the knowledge base.\"\n",
    "        \n",
    "        formatted_results = []\n",
    "        for i, result in enumerate(results.data, 1):\n",
    "            content = result.content if hasattr(result, 'content') else str(result)\n",
    "            formatted_results.append(f\"Result {i}: {content}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(formatted_results)\n",
    "    except Exception as e:\n",
    "        return f\"Error searching knowledge base: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def find_professors_by_expertise(topic: str) -> str:\n",
    "    \"\"\"Find professors who have expertise in a specific topic or subject area.\n",
    "    \n",
    "    Args:\n",
    "        topic: The topic or subject area to search for (e.g., 'Machine Learning', 'Quantum Physics', 'Botany')\n",
    "    \n",
    "    Returns:\n",
    "        List of professors with matching expertise, including their name, department, and contact info\n",
    "    \"\"\"\n",
    "    matching_profs = []\n",
    "    \n",
    "    for name, info in PROFESSORS.items():\n",
    "        # Check if the topic matches any of their expertise areas\n",
    "        if any(topic.lower() in exp.lower() or exp.lower() in topic.lower() for exp in info[\"expertise\"]):\n",
    "            matching_profs.append((name, info))\n",
    "    \n",
    "    if not matching_profs:\n",
    "        # If no exact match, show all professors\n",
    "        result = f\"No professors found with specific expertise in '{topic}'.\\n\\n\"\n",
    "        result += \"Available professors:\\n\\n\"\n",
    "        for name, info in PROFESSORS.items():\n",
    "            result += f\"**{name}** - {info['department']}\\n\"\n",
    "            result += f\"  Expertise: {', '.join(info['expertise'])}\\n\"\n",
    "            result += f\"  Email: {info['email']}\\n\\n\"\n",
    "        return result\n",
    "    \n",
    "    result = f\"Professors with expertise in '{topic}':\\n\\n\"\n",
    "    for name, info in matching_profs:\n",
    "        result += f\"**{name}** - {info['department']}\\n\"\n",
    "        result += f\"  Expertise: {', '.join(info['expertise'])}\\n\"\n",
    "        result += f\"  Email: {info['email']}\\n\\n\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Define all tools: custom tools + MCP binding\n",
    "tools = [\n",
    "    search_knowledge_base,\n",
    "    find_professors_by_expertise,\n",
    "    # MCP calendar server binding\n",
    "    {\n",
    "        \"type\": \"mcp\",\n",
    "        \"server_label\": \"canopy-calendar\",\n",
    "        \"server_url\": \"http://canopy-mcp-calendar-mcp-server:8080/sse\",\n",
    "        \"require_approval\": \"never\",\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Tools defined:\")\n",
    "print(\"   Custom tools:\")\n",
    "print(\"     - search_knowledge_base\")\n",
    "print(\"     - find_professors_by_expertise\")\n",
    "print(\"   MCP calendar server:\")\n",
    "print(\"     - canopy-calendar (provides 9 calendar tools via Responses API)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-section",
   "metadata": {},
   "source": [
    "## Build the Agent\n",
    "\n",
    "This is where the magic happens. With LangGraph, creating a ReAct agent is **one line of code**.\n",
    "\n",
    "No parsing. No loops. No state management. LangGraph handles it all.\n",
    "\n",
    "**What's happening behind the scenes:**\n",
    "- We pass the MCP calendar binding in the `tools` list\n",
    "- `create_react_agent()` automatically binds all tools (custom + MCP) to the LLM\n",
    "- The Responses API enables MCP tool binding, allowing direct integration with the MCP server\n",
    "- LangGraph handles all the async MCP function calls automatically\n",
    "- The agent can now interact with your actual calendar database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ReAct agent with all tools\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    tools,\n",
    "    checkpointer=MemorySaver(),\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ReAct agent created!\")\n",
    "print(\"\\nüéØ The agent has access to:\")\n",
    "print(\"   ‚Ä¢ 2 custom tools (RAG search, professor directory)\")\n",
    "print(\"   ‚Ä¢ 9 MCP calendar tools (automatically bound via Responses API)\")\n",
    "print(\"\\nüéØ Compare this to the 200+ lines you wrote before...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e41f723",
   "metadata": {},
   "source": [
    "We can see a small diagram of our agent as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e254a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what-happened",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "That one function call replaces ALL of this from your previous notebook:\n",
    "- ‚ùå `parse_react_response()` - Manual regex parsing\n",
    "- ‚ùå `execute_tool()` - Tool routing logic\n",
    "- ‚ùå `run_react_agent()` - The entire iteration loop\n",
    "- ‚ùå Conversation history management\n",
    "- ‚ùå Error handling and retry logic\n",
    "- ‚ùå Stopping condition checks\n",
    "\n",
    "LangGraph does **all of this automatically**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-section",
   "metadata": {},
   "source": [
    "## Test the Agent\n",
    "\n",
    "Let's see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe870fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from textwrap import indent\n",
    "from langchain_core.messages import HumanMessage\n",
    "import pprint\n",
    "\n",
    "def run_agent(question: str, thread_id: str | None = None):\n",
    "    \"\"\"Run the LangGraph agent, stream values, and show MCP tool calls + final answer.\"\"\"\n",
    "    thread_id = thread_id or str(uuid.uuid4())\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"USER INPUT\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    print(question, \"\\n\")\n",
    "\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    inputs = {\"messages\": [HumanMessage(content=question)]}\n",
    "\n",
    "    seen_messages = 0\n",
    "    final_state = None\n",
    "    tool_header_printed = False\n",
    "\n",
    "    # IMPORTANT: stream_mode=\"values\"\n",
    "    for state in agent.stream(inputs, config, stream_mode=\"values\"):\n",
    "\n",
    "        messages = state.get(\"messages\", [])\n",
    "        new_messages = messages[seen_messages:]\n",
    "        seen_messages = len(messages)\n",
    "\n",
    "        for msg in new_messages:\n",
    "            msg_type = getattr(msg, \"type\", None)\n",
    "\n",
    "            if msg_type == \"ai\":\n",
    "                if getattr(msg, \"tool_calls\", None):\n",
    "                    if not tool_header_printed:\n",
    "                        print(\"=\"*80)\n",
    "                        print(\"TOOL CALLS\")\n",
    "                        print(\"=\"*80 + \"\\n\")\n",
    "                        tool_header_printed = True\n",
    "\n",
    "                    for tc in msg.tool_calls:\n",
    "                        name = tc.get(\"name\")\n",
    "                        args = tc.get(\"args\")\n",
    "                        print(f\"üîß Calling tool: {name}\")\n",
    "                        print(f\"   Args: {args}\\n\")\n",
    "                else:\n",
    "                    tool_outputs = msg.additional_kwargs.get(\"tool_outputs\", [])\n",
    "\n",
    "                    # MCP tool calls live here\n",
    "                    for t in tool_outputs:\n",
    "                        if t.get(\"type\") == \"mcp_call\":\n",
    "                            if not tool_header_printed:\n",
    "                                print(\"=\"*80)\n",
    "                                print(\"TOOL CALLS\")\n",
    "                                print(\"=\"*80 + \"\\n\")\n",
    "                                tool_header_printed = True\n",
    "\n",
    "                            name = t.get(\"name\")\n",
    "                            server = t.get(\"server_label\")\n",
    "                            args = t.get(\"arguments\")\n",
    "                            error = t.get(\"error\", \"\")\n",
    "                            output = t.get(\"output\", \"\")\n",
    "\n",
    "                            print(f\"üîß MCP TOOL CALL: {name}  (server: {server})\")\n",
    "                            print(f\"   args: {args}\")\n",
    "                            if output:\n",
    "                                print(\"   output:\")\n",
    "                                print(indent(str(output), \"      \"))\n",
    "                            elif error:\n",
    "                                print(\"   error:\")\n",
    "                                print(indent(str(error), \"      \"))\n",
    "                            print()\n",
    "\n",
    "            elif msg_type == \"tool\":\n",
    "                # You can customize how much of the result to show\n",
    "                print(f\"üì¶ Tool result ({msg.name}): {str(msg.content)[:200]}...\\n\")\n",
    "\n",
    "        final_state = state\n",
    "\n",
    "    if final_state is None:\n",
    "        print(\"‚ö†Ô∏è Agent produced no state\")\n",
    "        return\n",
    "\n",
    "    # Final assistant message = last AI message\n",
    "    messages = final_state.get(\"messages\", [])\n",
    "    final_answer = None\n",
    "    for msg in reversed(messages):\n",
    "        if getattr(msg, \"type\", None) == \"ai\":\n",
    "            final_answer = msg\n",
    "            break\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"FINAL AGENT RESPONSE\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    if final_answer:\n",
    "        print(final_answer.content[0][\"text\"])\n",
    "    else:\n",
    "        print(\"(No assistant answer found)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example1",
   "metadata": {},
   "source": [
    "### Example 1: Knowledge Base Search\n",
    "\n",
    "The agent should search the knowledge base for this information.\n",
    "\n",
    "**Note**: This requires documents in your vector store. If you haven't done the RAG notebooks (`5-rag/2-intro-to-RAG.ipynb`), the knowledge base will be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test1",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(\"What is the structure of a forest canopy in botany?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example2",
   "metadata": {},
   "source": [
    "### Example 2: Finding an Expert & Scheduling\n",
    "\n",
    "When the agent can't answer a question, watch it:\n",
    "1. Search for professors with relevant expertise\n",
    "2. Schedule a calendar meeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d8dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(\"I need help understanding quantum chromodynamics. Can you find me an expert and schedule a meeting for December 1st, 2025 at 2pm for one hour?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example3",
   "metadata": {},
   "source": [
    "### Example 3: Using the MCP Calendar\n",
    "\n",
    "The agent can check your actual calendar and search for events!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2105f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(\"What upcoming lectures do I have?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "try-yourself",
   "metadata": {},
   "source": [
    "## Try It Yourself!\n",
    "\n",
    "Now experiment:\n",
    "1. Ask questions that require knowledge base searches\n",
    "2. Ask questions that should trigger meeting scheduling\n",
    "3. Try to make the agent use both tools in sequence\n",
    "\n",
    "See how the agent autonomously reasons about which tool(s) to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "try",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn!\n",
    "run_agent(\"YOUR QUESTION HERE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-root",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
