{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation with lm_eval\n",
    "\n",
    "This notebook demonstrates how to evaluate and compare quantized vs unquantized models using the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) library.\n",
    "\n",
    "We'll run MMLU (Massive Multitask Language Understanding) benchmarks against both models and compare their performance.\n",
    "\n",
    "**Note:** Multiple users may be running these tests simultaneously, so we use rate limiting to avoid overloading the model endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Setup\n",
    "\n",
    "Configure model endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Model endpoints - configure these for your environment\n",
    "os.environ[\"UNQUANTIZED_URL\"] = \"http://llama-32-predictor.ai501.svc.cluster.local:8080\"\n",
    "os.environ[\"UNQUANTIZED_MODEL\"] = \"llama32\"\n",
    "\n",
    "os.environ[\"QUANTIZED_URL\"] = \"http://llama-32-fp8-predictor.ai501.svc.cluster.local:8080\"\n",
    "os.environ[\"QUANTIZED_MODEL\"] = \"RedHatAI/Llama-3.2-3B-Instruct-FP8\"\n",
    "\n",
    "print(f\"Unquantized endpoint: {os.environ['UNQUANTIZED_URL']}\")\n",
    "print(f\"Unquantized model: {os.environ['UNQUANTIZED_MODEL']}\")\n",
    "print(f\"Quantized endpoint: {os.environ['QUANTIZED_URL']}\")\n",
    "print(f\"Quantized model: {os.environ['QUANTIZED_MODEL']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Understanding lm_eval\n",
    "\n",
    "The `lm-evaluation-harness` is a standard framework for evaluating language models. It supports:\n",
    "\n",
    "- **200+ benchmarks** including MMLU, HellaSwag, ARC, TruthfulQA, etc.\n",
    "- **Multiple model backends** including local models, OpenAI API, and vLLM endpoints\n",
    "- **Standardized evaluation** for reproducible comparisons\n",
    "\n",
    "For remote models served via OpenAI-compatible APIs (like vLLM), we use the `local-completions` model type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Evaluate Unquantized Model\n",
    "\n",
    "First, let's run MMLU evaluation on the unquantized (full precision) model.\n",
    "\n",
    "We use a single MMLU task with limited samples for a quick workshop demo. For production evaluations, remove the `limit` parameter and add more tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lm_eval\n",
    "import lm_eval.models.openai_completions\n",
    "from lm_eval.models.openai_completions import LocalCompletionsAPI\n",
    "\n",
    "# Fix for missing tqdm import in lm_eval's openai_completions module\n",
    "from tqdm import tqdm\n",
    "lm_eval.models.openai_completions.tqdm = tqdm\n",
    "\n",
    "# Also inject into the module's global namespace\n",
    "import sys\n",
    "sys.modules['lm_eval.models.openai_completions'].__dict__['tqdm'] = tqdm\n",
    "\n",
    "# Configure the unquantized model\n",
    "unquantized_model = LocalCompletionsAPI(\n",
    "    model=os.environ[\"UNQUANTIZED_MODEL\"],\n",
    "    base_url=f\"{os.environ['UNQUANTIZED_URL']}/v1/completions\",\n",
    "    num_concurrent=1,  # Limit concurrency to avoid overloading\n",
    "    tokenizer_backend=\"huggingface\",\n",
    "    tokenizer=\"RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\",\n",
    ")\n",
    "\n",
    "print(\"Unquantized model configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MMLU evaluation on unquantized model\n",
    "# Using limited samples for a quick workshop demo\n",
    "unquantized_results = lm_eval.simple_evaluate(\n",
    "    model=unquantized_model,\n",
    "    tasks=[\"mmlu_abstract_algebra\"],  # Single task for speed\n",
    "    num_fewshot=0,                     # No few-shot examples (faster)\n",
    "    batch_size=1,\n",
    "    limit=50,                          # Only evaluate 10 samples\n",
    ")\n",
    "\n",
    "print(\"Unquantized model evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display unquantized results\n",
    "print(\"Unquantized Model Results\")\n",
    "print(\"=\" * 50)\n",
    "for task, metrics in unquantized_results[\"results\"].items():\n",
    "    acc = metrics.get(\"acc,none\", metrics.get(\"acc\", \"N/A\"))\n",
    "    if isinstance(acc, float):\n",
    "        print(f\"{task}: {acc:.4f}\")\n",
    "    else:\n",
    "        print(f\"{task}: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Evaluate Quantized Model\n",
    "\n",
    "Now let's run the same evaluation on the FP8 quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the quantized model\n",
    "quantized_model = LocalCompletionsAPI(\n",
    "    model=os.environ[\"QUANTIZED_MODEL\"],\n",
    "    base_url=f\"{os.environ['QUANTIZED_URL']}/v1/completions\",\n",
    "    num_concurrent=1,  # Limit concurrency to avoid overloading\n",
    "    tokenizer_backend=\"huggingface\",\n",
    "    tokenizer=\"RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16\",\n",
    ")\n",
    "\n",
    "print(\"Quantized model configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MMLU evaluation on quantized model\n",
    "# Using same limited samples for comparison\n",
    "quantized_results = lm_eval.simple_evaluate(\n",
    "    model=quantized_model,\n",
    "    tasks=[\"mmlu_abstract_algebra\"],  # Single task for speed\n",
    "    num_fewshot=0,                     # No few-shot examples (faster)\n",
    "    batch_size=1,\n",
    "    limit=50,                          # Only evaluate 10 samples\n",
    ")\n",
    "\n",
    "print(\"Quantized model evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display quantized results\n",
    "print(\"Quantized Model Results\")\n",
    "print(\"=\" * 50)\n",
    "for task, metrics in quantized_results[\"results\"].items():\n",
    "    acc = metrics.get(\"acc,none\", metrics.get(\"acc\", \"N/A\"))\n",
    "    if isinstance(acc, float):\n",
    "        print(f\"{task}: {acc:.4f}\")\n",
    "    else:\n",
    "        print(f\"{task}: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Compare Results\n",
    "\n",
    "Let's compare the performance of both models side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "print(\"Model Comparison: Unquantized vs Quantized (FP8)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Task':<30} {'Unquantized':>15} {'Quantized':>15} {'Diff':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "total_unquant = 0\n",
    "total_quant = 0\n",
    "num_tasks = 0\n",
    "\n",
    "for task in unquantized_results[\"results\"].keys():\n",
    "    unquant_acc = unquantized_results[\"results\"][task].get(\"acc,none\", 0)\n",
    "    quant_acc = quantized_results[\"results\"][task].get(\"acc,none\", 0)\n",
    "    \n",
    "    if isinstance(unquant_acc, float) and isinstance(quant_acc, float):\n",
    "        diff = quant_acc - unquant_acc\n",
    "        print(f\"{task:<30} {unquant_acc:>15.4f} {quant_acc:>15.4f} {diff:>+10.4f}\")\n",
    "        total_unquant += unquant_acc\n",
    "        total_quant += quant_acc\n",
    "        num_tasks += 1\n",
    "\n",
    "if num_tasks > 0:\n",
    "    avg_unquant = total_unquant / num_tasks\n",
    "    avg_quant = total_quant / num_tasks\n",
    "    avg_diff = avg_quant - avg_unquant\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Average':<30} {avg_unquant:>15.4f} {avg_quant:>15.4f} {avg_diff:>+10.4f}\")\n",
    "    print()\n",
    "    print(f\"Quantization impact: {avg_diff*100:+.2f}% accuracy change\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "\n",
    "- How to use `lm_eval` to evaluate models served via OpenAI-compatible APIs\n",
    "- How to run MMLU benchmarks on remote model endpoints\n",
    "- How to compare quantized vs unquantized model performance\n",
    "- Best practices for rate limiting when multiple users share endpoints\n",
    "\n",
    "**Key Takeaways:**\n",
    "- FP8 quantization typically results in minimal accuracy loss (often <1%)\n",
    "- The trade-off is significant memory and inference speed improvements\n",
    "- Always benchmark on tasks relevant to your use case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
