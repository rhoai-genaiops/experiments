{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Introduction to LLM-Compressor ðŸ—œï¸\n",
    "\n",
    "[llm-compressor](https://github.com/vllm-project/llm-compressor) is the production quantization toolkit from the vLLM project. It provides a simple `oneshot` API for applying post-training quantization.\n",
    "\n",
    "In this notebook, you'll:\n",
    "1. Set up llm-compressor\n",
    "2. Quantize a small model using the `oneshot` API\n",
    "3. Compare the file size before and after quantization\n",
    "\n",
    "The example included below is designed to run on CPU rather than GPU.  It's a simple demonstration only of the process involved with compression.  Real world compression for enterprise use cases will involve much larger models and require multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Install dependencies. This notebook runs on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install-deps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "MODEL_ID = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "OUTPUT_DIR = \"qwen2-0.5b-W4A16\"\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "667dfb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                        2.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The Oneshot API\n",
    "\n",
    "The `oneshot` function is llm-compressor's main interface for quantization:\n",
    "\n",
    "```python\n",
    "oneshot(\n",
    "    model=\"model-name\",           # HuggingFace model ID\n",
    "    dataset=\"dataset-name\",       # Calibration dataset\n",
    "    recipe=recipe,                # Quantization configuration\n",
    "    output_dir=\"./output\",        # Where to save\n",
    "    num_calibration_samples=512,  # Samples for calibration\n",
    "    max_seq_length=4096,          # Sequence length\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sf8nygadazo",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Understanding Recipes\n",
    "\n",
    "A **recipe** defines how llm-compressor will quantize your model. It's a list of \"modifiers\" that specify the quantization algorithm and settings.\n",
    "\n",
    "### Available Modifiers\n",
    "\n",
    "llm-compressor provides several modifiers for different quantization approaches:\n",
    "\n",
    "**Quantization Modifiers:**\n",
    "\n",
    "| Modifier | Description |\n",
    "|----------|-------------|\n",
    "| `GPTQModifier` | GPTQ algorithm - uses calibration data to find optimal quantization values |\n",
    "| `AWQModifier` | Activation-aware Weight Quantization - preserves salient weights |\n",
    "| `AutoRoundModifier` | Intel's algorithm with learnable rounding/clipping |\n",
    "| `QuantizationModifier` | Basic PTQ and QAT for simple use cases |\n",
    "\n",
    "**Transform Modifiers** (for improving accuracy):\n",
    "\n",
    "| Modifier | Description |\n",
    "|----------|-------------|\n",
    "| `SmoothQuantModifier` | Smooths activations before quantization - often paired with GPTQ |\n",
    "| `QuIPModifier` | Hadamard rotations to reduce outliers |\n",
    "| `SpinQuantModifier` | SpinQuant-style rotations to even out weight distributions |\n",
    "\n",
    "Modifiers can be chained together. For example, applying `SmoothQuantModifier` before `GPTQModifier` can improve accuracy for W8A8 quantization.\n",
    "\n",
    "### GPTQModifier\n",
    "\n",
    "The most common modifier is `GPTQModifier`, which uses the GPTQ algorithm - a post-training quantization method that minimizes accuracy loss by using calibration data.\n",
    "\n",
    "```python\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "\n",
    "recipe = [\n",
    "    GPTQModifier(\n",
    "        scheme=\"W4A16\",       # Quantization scheme\n",
    "        targets=\"Linear\",    # Which layers to quantize\n",
    "        ignore=[\"lm_head\"],  # Layers to skip\n",
    "        group_size=128,      # Optional: weights per scale factor\n",
    "    ),\n",
    "]\n",
    "```\n",
    "\n",
    "### Quantization Schemes\n",
    "\n",
    "The `scheme` parameter determines the bit-width for weights (W) and activations (A):\n",
    "\n",
    "| Scheme | Weights | Activations | Size Reduction | Quality Impact |\n",
    "|--------|---------|-------------|----------------|----------------|\n",
    "| `W8A16` | 8-bit | 16-bit (FP16) | ~50% | Minimal |\n",
    "| `W4A16` | 4-bit | 16-bit (FP16) | ~75% | Low-Moderate |\n",
    "| `W8A8` | 8-bit | 8-bit | ~50% | Low |\n",
    "| `W4A8` | 4-bit | 8-bit | ~75% | Moderate |\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "| Parameter | Purpose | Recommendation |\n",
    "|-----------|---------|----------------|\n",
    "| `scheme` | Bit-width for weights/activations | Start with `W4A16` for good size/quality balance |\n",
    "| `targets` | Layer types to quantize | `\"Linear\"` targets all linear layers |\n",
    "| `ignore` | Layers to keep in full precision | Always include `[\"lm_head\"]` - the output layer is sensitive |\n",
    "| `group_size` | Weights sharing a scale factor | `128` (default) works well; smaller = better quality, larger model |\n",
    "\n",
    "### Why Our Example Configuration?\n",
    "\n",
    "```python\n",
    "GPTQModifier(\n",
    "    scheme=\"W4A16\",       # 4-bit weights give ~75% size reduction\n",
    "    targets=\"Linear\",     # Quantize all linear layers (most of the model)\n",
    "    ignore=[\"lm_head\"],   # Keep output layer precise for better predictions\n",
    ")\n",
    "```\n",
    "\n",
    "- **W4A16**: Aggressive weight compression while keeping activations in FP16 for stability\n",
    "- **targets=\"Linear\"**: Linear layers contain most parameters, so quantizing them gives the biggest savings\n",
    "- **ignore=[\"lm_head\"]**: The final output layer maps to vocabulary probabilities - keeping it in FP16 preserves output quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Quantize the Model\n",
    "\n",
    "Now let's apply our recipe to quantize `Qwen/Qwen2-0.5B-Instruct`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "quantize",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'sparse_logs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllmcompressor\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodifiers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mquantization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPTQModifier\n\u001b[32m      4\u001b[39m recipe = [\n\u001b[32m      5\u001b[39m     GPTQModifier(\n\u001b[32m      6\u001b[39m         scheme=\u001b[33m\"\u001b[39m\u001b[33mW4A16\u001b[39m\u001b[33m\"\u001b[39m,      \u001b[38;5;66;03m# or \"INT4\" / \"W4A8\" depending on your version\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     ),\n\u001b[32m     12\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43moneshot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwikitext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwikitext-2-raw-v1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_calibration_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mQuantized model saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.12/site-packages/llmcompressor/entrypoints/oneshot.py:329\u001b[39m, in \u001b[36moneshot\u001b[39m\u001b[34m(model, distill_teacher, config_name, tokenizer, processor, cache_dir, use_auth_token, precision, tie_word_embeddings, trust_remote_code_model, save_compressed, model_revision, recipe, recipe_args, clear_sparse_session, stage, dataset, dataset_config_name, dataset_path, splits, num_calibration_samples, shuffle_calibration_samples, max_seq_length, pad_to_max_length, text_column, concatenate_data, streaming, overwrite_cache, preprocessing_num_workers, min_tokens_per_module, calibrate_moe_context, quantization_aware_calibration, output_dir, log_dir, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# pass all args directly into Oneshot\u001b[39;00m\n\u001b[32m    326\u001b[39m local_args = {\n\u001b[32m    327\u001b[39m     k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m().items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mlocal_args\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mkwargs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    328\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m one_shot = \u001b[43mOneshot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlocal_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m one_shot()\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m one_shot.model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/app-root/lib64/python3.12/site-packages/llmcompressor/entrypoints/oneshot.py:122\u001b[39m, in \u001b[36mOneshot.__init__\u001b[39m\u001b[34m(self, log_dir, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# Set up logging\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m log_dir:\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     date_str = datetime.now().strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m_\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    124\u001b[39m     logger.add(\n\u001b[32m    125\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/oneshot_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.log\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    126\u001b[39m         level=\u001b[33m\"\u001b[39m\u001b[33mDEBUG\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    127\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:225\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: 'sparse_logs'"
     ]
    }
   ],
   "source": [
    "from llmcompressor import oneshot\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "\n",
    "recipe = [\n",
    "    GPTQModifier(\n",
    "        scheme=\"W4A16\",      # or \"INT4\" / \"W4A8\" depending on your version\n",
    "        targets=\"Linear\",\n",
    "        ignore=[\"lm_head\"],\n",
    "        # optional: group_size=128, act_order=True, etc.,\n",
    "        # depending on what llmcompressor exposes\n",
    "    ),\n",
    "]\n",
    "\n",
    "oneshot(\n",
    "    model=MODEL_ID,\n",
    "    dataset=\"wikitext\",\n",
    "    dataset_config_name=\"wikitext-2-raw-v1\",\n",
    "    recipe=recipe,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    max_seq_length=64,\n",
    "    num_calibration_samples=4,\n",
    ")\n",
    "\n",
    "print(f\"\\nQuantized model saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Compare File Sizes\n",
    "\n",
    "Let's compare the size of the quantized model to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "size-helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_size(path: str) -> int:\n",
    "    \"\"\"Get total size of files in a directory\"\"\"\n",
    "    p = pathlib.Path(path)\n",
    "    if not p.exists():\n",
    "        return 0\n",
    "    return sum(f.stat().st_size for f in p.rglob(\"*\") if f.is_file())\n",
    "\n",
    "def format_size(bytes: int) -> str:\n",
    "    \"\"\"Format bytes as human-readable string\"\"\"\n",
    "    if bytes < 1024:\n",
    "        return f\"{bytes} B\"\n",
    "    elif bytes < 1024**2:\n",
    "        return f\"{bytes/1024:.1f} KB\"\n",
    "    elif bytes < 1024**3:\n",
    "        return f\"{bytes/1024**2:.1f} MB\"\n",
    "    return f\"{bytes/1024**3:.2f} GB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-size",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get quantized model size\n",
    "size_q = folder_size(OUTPUT_DIR)\n",
    "\n",
    "print(\"Model Size Comparison\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Original (FP16):    ~1.0 GB\")\n",
    "print(f\"Quantized (W8A8):   {format_size(size_q)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list-files",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the quantized model files\n",
    "!ls -lh {OUTPUT_DIR}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2m9eu9251wk",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Test the Models\n",
    "\n",
    "Let's run a quick inference test to compare the original and quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5jcfu92yf4w",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "original_model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"cpu\")\n",
    "\n",
    "# Test prompt using chat template for proper formatting\n",
    "prompt = \"Complete this sentence: Model compression is good because\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "inputs = original_tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True)\n",
    "outputs = original_model.generate(inputs, max_new_tokens=100, do_sample=True, temperature=0.7, pad_token_id=original_tokenizer.eos_token_id)\n",
    "\n",
    "# Decode only the generated portion (skip the input tokens)\n",
    "generated_tokens = outputs[0][inputs.shape[-1]:]\n",
    "print(\"Original Model\")\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Response:\", original_tokenizer.decode(generated_tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4mumn5zsuto",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the quantized model\n",
    "import logging\n",
    "logging.getLogger(\"llmcompressor\").setLevel(logging.WARNING)\n",
    "\n",
    "quantized_tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR, device_map=\"cpu\")\n",
    "\n",
    "# Test with same prompt\n",
    "inputs = quantized_tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True)\n",
    "outputs = quantized_model.generate(inputs, max_new_tokens=100, do_sample=True, temperature=0.7, pad_token_id=quantized_tokenizer.eos_token_id)\n",
    "\n",
    "# Decode only the generated portion (skip the input tokens)\n",
    "generated_tokens = outputs[0][inputs.shape[-1]:]\n",
    "print(\"Quantized Model\")\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Response:\", quantized_tokenizer.decode(generated_tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rn097wa3p7",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Perplexity Comparison\n",
    "\n",
    "Perplexity measures how well a model predicts text - lower is better. Let's compare the original and quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pggvpc2x9t",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, dataset, max_tokens=5000, stride=512):\n",
    "    \"\"\"Calculate perplexity using sliding window evaluation.\"\"\"\n",
    "    # Tokenize entire dataset as one long sequence\n",
    "    encodings = tokenizer(\"\\n\\n\".join(dataset[\"text\"]), return_tensors=\"pt\")\n",
    "    input_ids = encodings.input_ids[:, :max_tokens]\n",
    "    \n",
    "    nlls = []\n",
    "    prev_end = 0\n",
    "    \n",
    "    for begin_loc in range(0, input_ids.size(1), stride):\n",
    "        end_loc = min(begin_loc + stride, input_ids.size(1))\n",
    "        trg_len = end_loc - prev_end\n",
    "        input_slice = input_ids[:, begin_loc:end_loc]\n",
    "        target_slice = input_slice.clone()\n",
    "        target_slice[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_slice, labels=target_slice)\n",
    "            neg_log_likelihood = outputs.loss * trg_len\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "        prev_end = end_loc\n",
    "\n",
    "    return math.exp(torch.stack(nlls).sum() / prev_end)\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "njljmeelvb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate perplexity for original model\n",
    "original_ppl = calculate_perplexity(original_model, original_tokenizer, test_dataset)\n",
    "print(f\"Original Model Perplexity: {original_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6srfevmbcyv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate perplexity for quantized model\n",
    "quantized_ppl = calculate_perplexity(quantized_model, quantized_tokenizer, test_dataset)\n",
    "print(f\"Quantized Model Perplexity: {quantized_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82zkwp15th",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "print(\"Perplexity Comparison\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Original Model:   {original_ppl:.2f}\")\n",
    "print(f\"Quantized Model:  {quantized_ppl:.2f}\")\n",
    "print(f\"Difference:       {quantized_ppl - original_ppl:+.2f} ({(quantized_ppl/original_ppl - 1)*100:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n7071e1q7b",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "You've learned:\n",
    "\n",
    "âœ… The `oneshot` API is llm-compressor's main interface for quantization\n",
    "\n",
    "âœ… `GPTQModifier` configures the quantization algorithm and scheme\n",
    "\n",
    "âœ… Key parameters: `scheme`, `targets`, `ignore`, `group_size`\n",
    "\n",
    "âœ… Quantization significantly reduces model size\n",
    "\n",
    "âœ… Quantized models produce comparable outputs to the original\n",
    "\n",
    "âœ… Perplexity testing measures the quality impact of quantization\n",
    "\n",
    "**Next:** Continue to [Advanced Quantization](./2-advanced-quantization.ipynb) to explore different schemes and compare trade-offs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app-root",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
