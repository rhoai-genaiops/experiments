{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your First Llama Stack Tutorial! üöÄ\n",
    "\n",
    "This notebook provides an introduction to **Llama Stack**, a unified platform for building AI applications with large models.\n",
    "\n",
    "Let's begin our journey!\n",
    "\n",
    "![](https://t4.ftcdn.net/jpg/05/65/74/01/360_F_565740157_DSKS34jAI4rQwnz697vXBcOvcYfEde2X.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Llama Stack client\n",
    "# the version has to match wit your Llama Stack Server\n",
    "!pip3 -q install llama-stack-client==0.3.0 fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.inference.event_logger import EventLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü¶ô Point to your Llama Stack Server\n",
    "\n",
    "We deployed a Llama Stack server previously through a helm chart in this project so we can directly call it with its deployment name and port, which was `llama-stack-service` and port `8321`.\n",
    "\n",
    "Therefore the base_url is just `http://llama-stack-service:8321`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"http://llama-stack-service:8321\"\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=base_url\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Provider Id\n",
    "Llama Stack server knows which model you'd like to send the requests from its run-config configuration. The configuration can be found in a configmap in your `userX-canopy`    namespace. In OpenShift console, you can expand `Workloads` from the left menu and click `ConfigMaps` to find `llama-stack-config` for more details.\n",
    "\n",
    "In order to communicate with the model through Llama Stack, we need to know model's `provider_id` in the Llama Stack config.\n",
    "\n",
    "In our case it is `llama32`. We passed this value as an environment variable during helm installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"vllm-llama32/llama32\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Your System Prompt\n",
    "Bring your system prompt here, update the below cell and start experimenting with Llama Stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt configures the assistant behavior\n",
    "sys_prompt = \"\"\"Summarize this\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is an example user message about Canopy. Let's summarize it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Message content\n",
    "user_message = \"\"\"\n",
    "Canopy (Biology)\n",
    "\n",
    "In biology and ecology, the canopy refers to the upper layer or ‚Äúroof‚Äù formed by the crowns of trees in a forest or wooded area. This layer plays a critical role in regulating the ecosystem by controlling light penetration, humidity, temperature, and wind flow within the forest environment. The canopy is typically made up of the tallest trees and their branches and leaves, which often form a dense, continuous cover that can be several meters thick.\n",
    "\n",
    "One of the primary ecological functions of the canopy is to provide habitat and food sources for a wide range of organisms. Many species of birds, insects, mammals, and epiphytes (plants that grow on other plants) are specially adapted to live in this elevated environment. The canopy also acts as a barrier that reduces the impact of heavy rain on the forest floor, helping to prevent soil erosion and maintain soil fertility.\n",
    "\n",
    "Moreover, the canopy plays a crucial role in photosynthesis on a large scale by capturing sunlight and converting it into chemical energy, which sustains the forest‚Äôs plant life and, consequently, the animals that depend on it. In tropical rainforests, the canopy is often so dense that very little sunlight reaches the forest floor, shaping the types of plants and animals that can survive in the understory and ground layers.\n",
    "\n",
    "Scientists study canopies using specialized tools and methods such as canopy cranes, drones, and climbing equipment to better understand their structure, biodiversity, and ecological functions. This knowledge is vital for conservation efforts, particularly as canopies are sensitive to deforestation, climate change, and human activities that threaten their integrity.\n",
    "\n",
    "Understanding the canopy‚Äôs complexity helps ecologists appreciate the interdependent relationships within forests and the critical services these ecosystems provide, including carbon storage, oxygen production, and climate regulation. Protecting the canopy is essential to maintaining biodiversity and the health of our planet.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a streaming request\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for log in EventLogger().log(response):\n",
    "    log.print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All you needed to do is provide your Llama Stack endpoint and the name of the model you want to query. If you introduce a new model or deploy a new version of an existing one, you simply update the provider ID, then the rest of your code remains unchanged. \n",
    "\n",
    "#### Llama Stack abstracts the communication with the model, so you don‚Äôt have to implement any model-specific logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to the instructions to bring Llama Stack into Canopy. So far, in Canopy, we were directly communicating with the model, but we need tis abstraction layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
